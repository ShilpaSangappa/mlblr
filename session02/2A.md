# Assignment 2A
*________________________________________________*

The approach used here is to programmatically replace the variable names, using a python script.

The file "CS231n-python-numpy-tutorial.ipynb" is downloaded using the "requests" module.

Because there are no patterns in the variable names, they are plain english text, there was no need of replacements using regular expressions.
The replace method from string module was one option. The drawback with this approach is it replaces all occurences of the variable in the string.

The requests module has the option to see the returned text in json format. This json format has been used to get the code blocks from the jupyter file individually and process and replace each code block separately.

The code for replacements is available in github at this location: [mlblr/Assignment2A - JSON.ipynb at master · ShilpaSangappa/mlblr · GitHub](https://github.com/ShilpaSangappa/mlblr/blob/master/Session%202/Assignment2A%20-%20JSON.ipynb)

The notebook file with replacements can be found at:[mlblr/JSON.ipynb at master · ShilpaSangappa/mlblr · GitHub](https://github.com/ShilpaSangappa/mlblr/blob/master/Session%202/JSON.ipynb)

# Assignment 2B
*________________________________________________*

The code to compute the new random variables and the corresponding other variables for these new random variables can be found at:

[mlblr/Assignment2B.ipynb at master · ShilpaSangappa/mlblr · GitHub](https://github.
com/ShilpaSangappa/mlblr/blob/master/Session%202/Assignment2B.ipynb)

### Imports


```python
import numpy as np
```

### Step 0: 
Read input and output


```python
X = np.array([[1,0,1,0], [1,0,1,1], [0,1,0,1]])
y = np.array([[1], [1], [0]])
print('X:\n');
print(X);
print('y:\n');
print(y);
```

    X:
    
    [[1 0 1 0]
     [1 0 1 1]
     [0 1 0 1]]
    y:
    
    [[1]
     [1]
     [0]]
    

### Step 1: 
Initialize weights and biases with random values (There are methods to initialize weights and biases but for now initialize with random values)


```python
#Generate 4*3 random numbers for the weights for first hidden layer
wh = np.random.rand(4,3)
print('wh:\n')
print(wh)
print('\n')

#Generate 1*3 random numbers for the bias for the first hidden layer
bh = np.random.rand(1,3)
print('bh:\n')
print(bh)
print('\n')

#Generate 1*3 random numbers for the weights for output layer
wout = np.random.rand(3,1)
print('wout:\n')
print(wout)
print('\n')

#Generate 1*1 random numbers for the bias for output layer
bout = np.random.rand(1,1)
print('bout:\n')
print(bout)
```

    wh:
    
    [[0.8074235  0.49593492 0.20706871]
     [0.34433857 0.49952714 0.2903024 ]
     [0.07491093 0.86213232 0.04414546]
     [0.05004834 0.73160642 0.90431809]]
    
    
    bh:
    
    [[0.30506553 0.32889076 0.53593608]]
    
    
    wout:
    
    [[0.29861991]
     [0.69748282]
     [0.69473639]]
    
    
    bout:
    
    [[0.55923121]]
    

### Step 2:
Calculate hidden layer input

hidden_layer_input = matrix_dot_product(X,wh) + bh


```python
hidden_layer_input = np.dot(X,wh) + bh
print('hidden_layer_input:\n')
print(hidden_layer_input)
```

    hidden_layer_input:
    
    [[1.18739997 1.686958   0.78715025]
     [1.23744831 2.41856442 1.69146835]
     [0.69945244 1.56002432 1.73055657]]
    

### Step 3: 
Perform non-linear transformation on hidden linear input.

hiddenlayer_activations = sigmoid(hidden_layer_input)


```python
def sigmoid(x):
    return 1/(1 + np.exp(-x))

hiddenlayer_activations = sigmoid(hidden_layer_input)
print('hiddenlayer_activations:\n')
print(hiddenlayer_activations)
```

    hiddenlayer_activations:
    
    [[0.76627573 0.84382369 0.68721911]
     [0.77511954 0.91823202 0.84441716]
     [0.66806636 0.82635684 0.8494836 ]]
    

### Step 4: 

Perform linear and non-linear transformation of hidden layer activation at output layer

##### Output layer Summation:
output_layer_input = matrix_dot_product (hiddenlayer_activations * wout ) + bout 

##### Output layer Activation:
output = sigmoid(output_layer_input)



```python
output_layer_input = np.dot(hiddenlayer_activations,wout) + bout
output = sigmoid(output_layer_input)
print('output:\n')
print(output)
```

    output:
    
    [[0.86460134]
     [0.88265289]
     [0.87272451]]
    

### Step 5:
Calculate gradient of Error(E) at output layer

E = y-output


```python
E = y-output
print('E:\n')
print(E)
```

    E:
    
    [[ 0.13539866]
     [ 0.11734711]
     [-0.87272451]]
    

### Step 6: 
Compute slope at output and hidden layer

Slope_output_layer= derivatives_sigmoid(output)

Slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)`


```python
def derivatives_sigmoid(x):
    return x*(1-x)
slope_output_layer= derivatives_sigmoid(output)
print('slope_output_layer:\n')
print(slope_output_layer)

slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)
print('slope_hidden_layer:\n')
print(slope_hidden_layer)
```

    slope_output_layer:
    
    [[0.11706586]
     [0.10357677]
     [0.11107644]]
    slope_hidden_layer:
    
    [[0.17909724 0.13178527 0.21494901]
     [0.17430924 0.07508197 0.13137682]
     [0.2217537  0.14349121 0.12786121]]
    

### Step 7:
Compute delta at output layer

d_output = E * slope_output_layer*lr



```python
lr = 1
d_output = E * slope_output_layer*lr
print('d_output:\n')
print(d_output)

print('\nlr:\n')
print(lr)
```

    d_output:
    
    [[ 0.01585056]
     [ 0.01215443]
     [-0.09693913]]
    
    lr:
    
    1
    

### Step 8: 
Calculate Error at hidden layer

Error_at_hidden_layer = matrix_dot_product(d_output, wout.Transpose)


```python
Error_at_hidden_layer = np.dot(d_output, wout.T)
print('Error_at_hidden_layer:\n')
print(Error_at_hidden_layer)
```

    Error_at_hidden_layer:
    
    [[ 0.00473329  0.01105549  0.01101196]
     [ 0.00362956  0.00847751  0.00844413]
     [-0.02894795 -0.06761338 -0.06734714]]
    

### Step 9:
Compute delta at hidden layer

d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer


```python
d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer
print('d_hiddenlayer:\n')
print(d_hiddenlayer)
```

    d_hiddenlayer:
    
    [[ 0.00084772  0.00145695  0.00236701]
     [ 0.00063267  0.00063651  0.00110936]
     [-0.00641932 -0.00970193 -0.00861109]]
    

### Step 10: 
Update weight at both output and hidden layer


```python
wout = wout + np.dot(hiddenlayer_activations.T, d_output) * lr

wh = wh+ np.dot(X.T,d_hiddenlayer) * lr
print('wout:\n')
print(wout)
print('wh:\n')
print(wh)
```

    wout:
    
    [[0.25542518]
     [0.64191218]
     [0.6335444 ]]
    wh:
    
    [[0.80890389 0.49802838 0.21054508]
     [0.33791925 0.48982521 0.28169131]
     [0.07639132 0.86422578 0.04762183]
     [0.04426169 0.72254101 0.89681637]]
    

### Step 11: 
Update biases at both output and hidden layer

bh = bh + sum(d_hiddenlayer, axis=0) * learning_rate

bout = bout + sum(d_output, axis=0)*learning_rate


```python
bh = bh + np.sum(d_hiddenlayer, axis=0) * lr

bout = bout + np.sum(d_output, axis=0)*lr
print('bh:\n')
print(bh)
print('bout:\n')
print(bout)
```

    bh:
    
    [[0.3001266  0.3212823  0.53080137]]
    bout:
    
    [[0.49029707]]
    
